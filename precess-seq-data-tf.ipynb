{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e45c24e6-a2c2-4998-a9b1-0a972b36abc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import esm\n",
    "from esm import Alphabet, FastaBatchedDataset, pretrained\n",
    "from esm.pretrained import esm2_t30_150M_UR50D\n",
    "from CCMpred import CCMPredEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from Bio import SeqIO\n",
    "from typing import Iterable, List, Optional, Sequence, Tuple\n",
    "import pickle\n",
    "import math\n",
    "from numpy2tfrecord import Numpy2TFRecordConverter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4369b78-358c-4aec-8806-0fe22380a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2reps(FASTA_PATH):\n",
    "    model, alphabet = esm2_t30_150M_UR50D()\n",
    "    model.eval()\n",
    "    model = model.cuda()\n",
    "    dataset = FastaBatchedDataset.from_file(FASTA_PATH)\n",
    "    batches = dataset.get_batch_indices(64, extra_toks_per_seq=1)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, collate_fn=alphabet.get_batch_converter(1024), batch_sampler=batches\n",
    "    )\n",
    "    print(f\"{len(dataset)} sequences\")\n",
    "    repr_layers = [model.num_layers]\n",
    "    Xs_glo = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (labels, strs, toks) in enumerate(data_loader):\n",
    "            toks = toks.to(device=\"cuda\", non_blocking=True)\n",
    "            out = model(toks, repr_layers=repr_layers)\n",
    "            logits = out[\"logits\"].to(device=\"cpu\")\n",
    "            representations = {\n",
    "                layer: t.to(device=\"cpu\") for layer, t in out[\"representations\"].items()}\n",
    "            for i, label in enumerate(labels):\n",
    "                result = {\"num\": label}\n",
    "                truncate_len = len(strs[i])\n",
    "                result[\"representations\"] = {\n",
    "                    layer: t[i, 1 : truncate_len + 1].clone()\n",
    "                    for layer, t in representations.items()}\n",
    "                Xs_glo.append(result['representations'][model.num_layers])\n",
    "    Xs_glo = torch.stack(Xs_glo, dim=0)\n",
    "    return Xs_glo\n",
    "\n",
    "def read_seqs(filename: str, nseq: int) -> List[Tuple[str, str]]:\n",
    "    records_raw = list(SeqIO.parse(filename, \"fasta\"))\n",
    "    records = [(x.description, str(x.seq)) for x in records_raw]\n",
    "    if len(records) <= nseq:\n",
    "        return records\n",
    "    return records[:nseq]\n",
    "\n",
    "def seq2loc(FASTA_PATH,num_seqs,braw):\n",
    "    sequences=[]\n",
    "    seqs_data = read_seqs(FASTA_PATH, num_seqs)\n",
    "    for num, seq in seqs_data:\n",
    "        sequences.append(seq)\n",
    "    encoder = CCMPredEncoder(brawfile=braw, seq_len=len(seqs_data[0][1]))\n",
    "    matrix = encoder.encode(sequences)\n",
    "    loc = torch.from_numpy(matrix).float()\n",
    "    return loc\n",
    "    \n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "def l2_norm_tns(tns):\n",
    "    \"\"\" l2 normalize the tns \"\"\"\n",
    "    return tns / tns.norm(dim=-1,p=2,keepdim=True)\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, FASTA_PATH, DATA_PATH,num_seqs,braw,graph_reps_path):\n",
    "        self.glo = seq2reps(FASTA_PATH)\n",
    "        self.loc = seq2loc(FASTA_PATH,num_seqs,braw)\n",
    "        #self.glo = torch.Tensor(self.glo)\n",
    "        #self.loc = torch.Tensor(self.loc)\n",
    "        #self.norm = nn.Sequential(LambdaLayer(l2_norm_tns))\n",
    "        #self.glo_norm = self.norm(self.glo)\n",
    "        #self.loc_norm = self.norm(self.loc)\n",
    "        #self.feature = torch.cat((self.glo_norm,self.loc_norm),dim=2)\n",
    "        #lable\n",
    "        self.score = pd.read_csv(DATA_PATH, sep=',')\n",
    "        self.Ys = pd.DataFrame(self.score['DMS_score'])\n",
    "        self.Ys_array = np.array(self.Ys)\n",
    "        self.y_score = self.Ys_array.squeeze(1)\n",
    "        self.y = torch.tensor(self.y_score).float()\n",
    "        self.lable = self.y\n",
    "        #graph\n",
    "        self.graph_reps = pd.read_csv(graph_reps_path)\n",
    "        self.graph_reps = np.array(self.graph_reps)\n",
    "        self.graph_reps = np.delete(self.graph_reps,0,axis=1)\n",
    "        self.graph_reps = torch.Tensor(self.graph_reps)\n",
    "        self.norm = nn.Sequential(LambdaLayer(l2_norm_tns))\n",
    "        self.graph_norm = self.norm(self.graph_reps)\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.glo[idx],self.loc[idx],self.lable[idx],self.graph_norm[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lable)\n",
    "\n",
    "def dump_large_file(data_file, chunk_size, dump_file):\n",
    "    with open(dump_file, 'ab') as f:\n",
    "        for i in range(0, len(data_file), chunk_size):\n",
    "            a1=math.modf(len(data_file)/chunk_size)\n",
    "            if i< int(a1[1])*chunk_size:\n",
    "                chunk = data_file[i:i+chunk_size]\n",
    "                pickle.dump(chunk, f)\n",
    "            else:\n",
    "                chunk = data_file[i:i+int((a1[0]*chunk_size))]\n",
    "                pickle.dump(chunk, f)\n",
    "\n",
    "def load_large_file(load_file):\n",
    "    with open(load_file, 'rb') as f:\n",
    "        while True:\n",
    "            try:\n",
    "                yield pickle.load(f)\n",
    "            except EOFError:\n",
    "                break\n",
    "\n",
    "def data2pkl(FASTA_PATH, DATA_PATH,num_seqs,braw, graph_reps_path):\n",
    "    glo = seq2reps(FASTA_PATH)\n",
    "    loc = seq2loc(FASTA_PATH,num_seqs,braw)\n",
    "    print(loc.shape)\n",
    "    #norm = nn.Sequential(LambdaLayer(l2_norm_tns))\n",
    "    #glo_norm = norm(glo)\n",
    "    #loc_norm = norm(loc)\n",
    "    #feature = torch.cat((glo_norm,loc_norm),dim=2)\n",
    "    #lable\n",
    "    score = pd.read_csv(DATA_PATH, sep=',')\n",
    "    Ys = pd.DataFrame(score['DMS_score'])\n",
    "    Ys_array = np.array(Ys)\n",
    "    y_score = Ys_array.squeeze(1)\n",
    "    y = torch.tensor(y_score).float()\n",
    "    lable = y\n",
    "    #graph\n",
    "    graph_reps = pd.read_csv(graph_reps_path)\n",
    "    graph_reps = np.array(graph_reps)\n",
    "    graph_reps = np.delete(graph_reps,0,axis=1)\n",
    "    graph_reps = torch.Tensor(graph_reps)\n",
    "    norm = nn.Sequential(LambdaLayer(l2_norm_tns))\n",
    "    graph_norm = norm(graph_reps)\n",
    "    return glo, loc, lable, graph_norm\n",
    "\n",
    "def data_loader(FASTA_PATH, DATA_PATH, num_seqs,braw,graph_reps_path):\n",
    "    dataset = SeqDataset(FASTA_PATH, DATA_PATH, num_seqs,braw,graph_reps_path)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    #train_loader = DataLoader(train_dataset,batch_size=32,shuffle=True,pin_memory=True,num_workers=2)\n",
    "    #test_loader = DataLoader(test_dataset,batch_size=32, shuffle=False)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def split(FASTA_PATH, DATA_PATH, num_seqs,braw,graph_reps_path):\n",
    "    glo, loc, lable, graph = data2pkl(FASTA_PATH, DATA_PATH, num_seqs,braw,graph_reps_path)\n",
    "    train_size = 0.8\n",
    "    seed = 42\n",
    "    glo_train, glo_test, loc_train, loc_test, lable_train, lable_test, graph_train, graph_test = train_test_split(glo, loc, lable, graph, train_size=train_size, random_state=seed)\n",
    "    return glo_train, glo_test, loc_train, loc_test, lable_train, lable_test, graph_train, graph_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acfab491-e5b3-423b-aca5-a29d740b2155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2695 sequences\n",
      "torch.Size([2695, 163, 164])\n"
     ]
    }
   ],
   "source": [
    "FASTA_PATH = \"./data/seqs.fasta\" \n",
    "DATA_PATH = './data/VKOR1_HUMAN_Chiasson_abundance_2020.csv'\n",
    "braw='./data/VKOR1.braw'\n",
    "graph_reps_path = './VKOR1-mc.csv'\n",
    "EMB_LAYER = 30\n",
    "num_seqs = 100000\n",
    "glo_train, glo_test, loc_train, loc_test, lable_train, lable_test, graph_train, graph_test = split(FASTA_PATH, DATA_PATH, num_seqs,braw,graph_reps_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "776cd684-9151-4b88-97a3-a1812c934f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Numpy2TFRecordConverter(\"./data/tf_train.tfrecord\") as converter:\n",
    "    samples = {\n",
    "        \"glo\": glo_train.numpy().astype(np.float32),\n",
    "        \"loc\": loc_train.numpy().astype(np.float32),\n",
    "        \"lable\": lable_train.numpy().astype(np.float32),\n",
    "        \"graph\": graph_train.numpy().astype(np.float32),\n",
    "    }  # batch of 32 samples\n",
    "\n",
    "    converter.convert_batch(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82b3b00f-9e88-4905-b539-23e764f67400",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Numpy2TFRecordConverter(\"./data/tf_test.tfrecord\") as converter:\n",
    "    samples = {\n",
    "        \"glo\": glo_test.numpy().astype(np.float32),\n",
    "        \"loc\": loc_test.numpy().astype(np.float32),\n",
    "        \"lable\": lable_test.numpy().astype(np.float32),\n",
    "        \"graph\": graph_test.numpy().astype(np.float32),\n",
    "    }  # batch of 32 samples\n",
    "\n",
    "    converter.convert_batch(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629f9523-1c2e-4b33-9c51-d0289f489b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
